# Interactive Conversation Protocol

## 1. 프로젝트 정의

Interactive Conversation Protocol (ICP)는 지시적 의미를 설명하기 위한 구조화된 프로토콜입니다. 클라이언트는 이 프로토콜의 구조에 따라 지시적 의미를 인간이 인식 가능한 모듈식 멀티모달 메시지로 결합할 수 있습니다.

**핵심 포지셔닝**: 인간-기계 상호작용을 위한 중간 언어 형태로, 본질적으로 개념 그래프의 구조화된 텍스트 표현이며, 이를 "Conception Annotation(개념 주석)"이라고 합니다.

## 2. 이 프로젝트가 생성된 이유

### ❓ 해결해야 할 문제

현실에 직면해야 합니다: 상당한 기간 동안 사용자와 상호작용하는 방식은 엔드사이드 개발자(또는 회사)에 의해 결정됩니다. 대부분의 기존 비즈니스 모델에서 사용자의 상호작용 참여는 제품 가치와 수익성의 기초이며, 활성 사용자 수 및 광고 수익 등이 포함됩니다. 아무도 엔드사이드에 충분한 권한을 열도록 강제할 수 없으며, 인간의 개입 없이 AI가 완전히 작업을 실행하도록 허용할 수 없습니다.

AI가 충분히 똑똑하다면, 인간은 실제로 매번 홈페이지에서 시작할 필요가 없습니다. 따라서 인간-기계 대화가 차세대 주요 상호작용 인터페이스가 되는 것은 거의 합의가 되었음을 볼 수 있습니다.

그러나 자연어 표현력의 자연스러운 결함은 원래 잘 설계된 상호작용으로 보완되기를 기대했지만, 이제 대화 상자로 대체되었습니다. 대화 상자의 한계가 즉시 드러납니다:

#### (1) 커서의 지시 기능 손실

상호작용 형태가 "화면 + 포커스 작업" 모드에서 자연어 모드로 전환되고 있습니다. 전통적인 포커스 작업은 키보드, 마우스 및 터치스크린을 통해 달성되며 정확한 지시를 제공합니다. 자연어 상호작용은 다음과 같은 영향을 미칩니다:

- **지시 정확도의 손실**: 표현과 이해의 어려움이 증가하고 모호성이 증가하며, 이를 "커서 손실 효과"라고 합니다.
  > 예를 들어, 사용자가 "이것을 삭제"라고 말할 때 시스템은 "이것"이 어떤 특정 객체를 가리키는지 판단하기 어렵지만, 전통적인 인터페이스는 마우스 클릭을 통해 정확하게 위치를 지정할 수 있습니다.

- **정보 표현 효율성 제한**: 순수 음성 정보 표현은 비효율적이며, 음성 입력의 장점은 주로 단어별 표현 시나리오에 반영됩니다.
  > 예를 들어, 썸네일을 확대하려면 "확대"라고 말하거나 "확대"를 입력해야 할 수 있지만, 전통적인 상호작용은 한 번의 클릭만 필요합니다.

- **언어적 표현 능력에 대한 높은 요구사항**: 자연어 상호작용은 사용자의 언어적 표현 능력에 높은 요구를 부과하여 인간-기계 상호작용에 어려움을 만듭니다.
  > 예를 들어, 언어적 표현에 능숙하지 않은 사용자는 자신의 요구를 정확하게 설명할 수 없어 시스템 이해의 편차를 초래할 수 있지만, 전통적인 인터페이스는 버튼, 메뉴 등의 시각적 요소를 통해 표현의 문턱을 낮춥니다.

- **정보 읽기 효율성 저하**: 텍스트 스트림 읽기와 음성 읽기는 구조화된 정보 읽기만큼 효율적이지 않습니다.
  > 예를 들어, 시스템이 음성으로 긴 데이터 목록을 방송할 때 사용자는 목표 정보를 찾기 위해 전체 목록을 들어야 하지만, 전통적인 인터페이스는 테이블, 카드 등의 구조화된 형태를 통해 사용자가 빠르게 스캔하고 위치를 지정할 수 있게 합니다.

- **대화 턴에 제약됨**: 대화 턴에 제약된 상호작용은 빠른 연속 작업에 친화적이지 않습니다.
  > 예를 들어, 사용자가 여러 작업을 연속적으로 수행해야 할 때 다음 단계로 진행하기 전에 각 대화 턴이 완료될 때까지 기다려야 하지만, 전통적인 인터페이스는 여러 버튼을 빠르게 연속 클릭하여 일괄 작업을 완료할 수 있습니다.

#### (2) 정보 조각화 오버플로우

대화의 스트리밍 정보 구조는 조직화가 부족하며, 페이지 단위로 정보 아키텍처를 구성하고 시각적 그래픽 인터페이스를 통해 시각적으로 친화적인 정보 프레젠테이션 계층을 구축하는 전통적인 소프트웨어와 다릅니다. 이는 다음과 같은 파생 문제로 이어집니다:

- **다른 정보 분리의 어려움**: 단일 대화 내의 연속적인 정보 흐름은 다른 주제 간의 경계를 구별하기 어렵게 만들며, 여러 완전히 무관한 주제가 혼합될 수 있습니다.
  > 예를 들어, 사용자가 대화에서 먼저 "내일 날씨를 확인해줘"라고 요청한 다음 "그 프로젝트 진행 상황은 어때"라고 묻고, 그 다음 "좋은 책 몇 권 추천해줘"라고 요청합니다. 이러한 완전히 무관한 주제가 혼합되어 빠르게 위치를 지정하고 검토하기 어렵습니다.

- **좀비 세션 폭발**: 정보가 세션을 통해 인위적으로 분리될 때, 세션 내의 정보는 세션을 단위로 하는 블랙박스로 접혀 결국 낮은 가시성으로 인해 좀비 세션이 됩니다.
  > 예를 들어, 사용자는 "작업 관련", "학습 노트", "쇼핑 목록"과 같은 여러 세션을 생성하지만 각 세션에는 흩어진 메시지만 있습니다. 시간이 지나면 이러한 세션은 잊혀지고 효과적으로 활용할 수 없는 좀비 세션이 됩니다.

- **다차원적으로 관리할 수 없음**: 무수한 세션에 흩어진 유사한 정보는 특정 차원을 따라 정보를 관리할 수 없기 때문에 구성할 수 없습니다.
  > 예를 들어, 사용자가 다른 세션에서 "Python 튜토리얼", "JavaScript 튜토리얼", "React 튜토리얼" 및 기타 학습 리소스에 대해 물었지만 "학습 리소스" 차원을 따라 균일하게 보고 관리할 수 없으며 세션별로만 검색할 수 있습니다.

- **지시 가능한 객체 부족**: 정보는 텍스트 정보에 용해되며, 무언가를 참조해야 할 때 참조할 특정 객체가 없습니다.
  > 예를 들어, 사용자가 "그 제안을 다시 최적화해줘"라고 말할 때 "그 제안"은 독립적인 식별과 구조가 없는 텍스트 스트림 내의 단락일 뿐이며, 시스템이 정확하게 위치를 지정하고 작업하기 어렵게 만듭니다.

#### (3) 다른 터미널 간 인간-기계 인터페이스의 큰 차이

미래에 더 많은 터미널 장치가 에이전트에 의해 구동되며, 화면, 카메라, 마이크, 스피커 및 기타 장치를 통해 인간의 지각에 대응하여 인간-기계 상호작용을 완료합니다. 그러나 다른 터미널은 물리적 특성에 고유한 차이가 있으며 동일한 상호작용 모드를 강제로 사용하는 것은 불가능합니다. 이는 AI 통합에 어려움을 만듭니다:

- **미디어 단절**: AI에 의해 피드백된 정보 구조가 터미널에 대해 불친절할 때, 정보 표현의 손실이나 혼란을 필연적으로 초래합니다. 반대로 터미널에 의해 제공되는 정보 구조가 반드시 AI 친화적인 것은 아닙니다.
  > 예를 들어, 원래 대형 화면 대시보드용으로 설계된 복잡한 데이터 시각화가 스마트 스피커에서 음성으로 직접 "읽혀지면" 사용자가 전체 인식을 확립하는 것이 거의 불가능합니다. 반대로 스마트워치의 단일 라인 프롬프트 정보는 AI가 표현하기를 기대하는 복잡한 의미론을 완전히 전달하기 어렵습니다.

- **AI가 터미널 특성을 충분히 숙달하지 못함**: 표현력을 향상시키기 위해 인간은 복잡한 맥락에서 또는 복잡한 논리를 표현할 때 여러 소프트웨어와 터미널을 사용하여 시연하는 경우가 많습니다. AI는 "말하는" 방법만 아는 것 같습니다.
  > 예를 들어, 제품 관리자가 제안을 제시할 때 슬라이드를 보여주고, 화이트보드에 구조 다이어그램을 그리고, 데모 페이지에서 클릭 작업을 수행합니다. 반면 현재 AI는 종종 긴 텍스트 또는 음성 문자열로만 설명할 수 있어 투영, 주석, 애니메이션과 같은 터미널 기능을 사용하여 표현을 향상시키기 어렵습니다.

- **가상과 현실 사이의 격차**: 현재 AI가 사용하는 맥락(또는 컨텍스트)은 사전 설정되고 기억된 지식에 기반하며, 실제 시나리오의 맥락은 종종 동적이고 실제 환경과 관련이 있습니다.
  > 예를 들어, AI는 사용자의 개인 프로필과 역사적 대화를 "기억"할 수 있지만, 사용자가 회의실에 앉아 있는지, 종이 문서의 어느 페이지를 넘기고 있는지, 또는 어떤 물리적 디스플레이 보드를 가리키고 있는지를 실시간으로 인식하기 어렵습니다. 따라서 실제 어시스턴트처럼 현장 상황에 기반하여 자연스러운 지시와 보완을 할 수 없습니다.

### 💡 개선 아이디어 및 목표

이전에는 제품 관리자의 주요 작업이 배우기 쉽고 사용하기 쉬운 인터페이스와 작업 흐름을 설계하는 것이었습니다. AI 지원으로 사용자는 더 이상 소프트웨어 상호작용 인터페이스와 작업 로직을 학습할 필요가 없습니다. AI는 사용자의 질문과 지시에 따라 사용자에게 필요한 정보만 제공하는 능력을 가지며, 사용자는 최소한의 개입 작업만 필요로 합니다.

그러나 사용자 자신이 개입하는 한, 상호작용 친화성, 정확성 및 효율성에 문제가 있습니다. Interactive Conversation Protocol은 인간-기계 접촉 지점에서 정확히 역할을 합니다:

#### 자연어의 표현력 향상 (인간 → AI)

여기서 표현력 향상은 자연어 향상을 의미합니다. 위에서 언급한 문제(커서 지시의 손실, 정보 조각화 오버플로우, 다른 터미널 간 인간-기계 인터페이스의 차이)를 보완하기 위해. 최소한 원래 자연어에 대해 다음 처리를 수행할 수 있습니다:

- **표현된 정보 마킹**: 특별한 처리가 필요한 정보를 표시합니다. 여기서 언급한 특별한 처리에는 구조화된 정보 사용, 인터페이스 조립, 보조 프로그램 실행 등이 포함됩니다. 텍스트 조각에 점을 둘러싸서 메모를 만드는 것으로 상상할 수 있습니다. 마킹 형태 측면에서 우리는 Markdown을 참조하며, 특수 문자를 사용하여 특정 의미를 나타내고, 보조 기능의 설명 및 트리거는 Java 개발의 주석 원칙을 참조합니다. 이 방법을 통해 원래 설명 내용에 말투를 보완하고, 무엇이 중요한지, 무엇이 특별한 프레젠테이션 형태를 필요로 하는지, 무엇이 사전 작업을 필요로 하는지(예: 인증이 자신에게만 보이는 것)를 지적할 수 있습니다.
  > 예를 들어, 사용자가 "이번 주 할 일을 정리해줘"라고 말할 때, 문장에서 날짜, 우선순위 및 책임자를 가볍게 표시하면 AI는 설명 텍스트를 반환하는 대신 확인 가능한 할 일 목록을 직접 생성할 수 있습니다.

- **컨텍스트 정보 추가**: 화자의 실제 상황을 재현하기 위해 필요한 가상 정보와 실제 세계 환경을 서술 정보에 보완합니다. 전통적인 상호작용 인터페이스는 종종 인터페이스에 선택적 컨텍스트 정보를 사전 설정하여 사용자의 간단한 클릭에서 정확한 의도를 캡처하는 반면, 자연어는 컨텍스트를 완전히 설명하기 위해 긴 텍스트를 구성해야 합니다. 프로토콜에 시간, 위치, 장치 상태, 참가자 신원 등의 컨텍스트 정보를 보완함으로써 AI는 "여기 지금"의 실제 의미를 더 정확하게 이해할 수 있습니다.
  > 예를 들어, 사용자가 "근처에 Marry가 좋아하는 레스토랑을 예약해줘"라고만 말할 때, 위치, 예산 선호도 및 역사적 주문을 컨텍스트 정보로 보완합니다. 컨텍스트 정보의 적용은 매우 광범위하며, 나중에 시나리오에 대해 구체적으로 논의할 것입니다.

- **표준 중간 언어로 번역**: 원래 정보를 처리한 후(주석 및 컨텍스트 정보 추가), 완전하고 정확한 해석을 가능하게 하려면 합의된 데이터 식별 시스템이 필요합니다. 모든 터미널의 표현력에 적응하기 위해 이 식별 시스템은 JSON 사양에 구축할 수 있으며 합의된 매개변수 테이블 및 구조를 제공합니다. 이렇게 하면 다양한 수신 단의 AI는 사용 가능한 모든 터미널을 동원하여 최대 표현력을 보여주고 표현자의 완전한 의미를 재현할 수 있습니다.
  > 예를 들어, "이 구절을 프로젝트 그룹에 보내고 오늘 근무 종료 전에 모든 사람이 확인하도록 하라"는 문장은 최종적으로 메시지 본문, 수신자 목록, 마감일 및 확인 버튼 구성을 포함하는 표준 JSON 구조로 번역됩니다. 채팅 도구, 웹 백엔드 또는 모바일 앱은 모두 그에 따라 각각 적응된 인터페이스를 렌더링할 수 있습니다.

#### 수요에 맞춘 맞춤형 인터페이스 (AI → 인간)

우리의 전제는 사람들이 "말하기"를 통해 AI와 상호작용하는 것을 선호할 것이라는 것입니다. 이는 인간 커뮤니케이션에 가장 가까운 방법입니다. 따라서 사람들은 클릭을 통해 필요한 기능 인터페이스를 찾는 것이 점점 더 번거롭다고 느낄 것입니다. 사람들이 필요로 하는 정보와 인터페이스는 사용자의 "눈"에 직접 푸시되어야 합니다. 이 효과를 달성하기 위해 수신 단은 특정 해석 능력을 가져야 합니다:

- **중간 언어 해석**: 중간 언어가 JSON 형식이므로 모든 수신 단은 완전한 의미를 읽을 수 있으며, 최소한 정보 수신에서 단절을 피할 수 있습니다.
  > 예를 들어, "비용 보고서 검토 요청"의 동일한 중간 언어 데이터는 데스크톱에서 테이블 및 첨부 파일 미리보기가 있는 대형 화면 인터페이스로 렌더링될 수 있으며, 모바일에서는 주요 정보와 두 개의 버튼(승인/거부)만 표시되며, 스마트 스피커는 요약을 읽고 음성 확인을 기다릴 수 있습니다.

- **메시지 인터페이스 동적 구성**: 완전한 컨텍스트 및 주석을 기반으로 가장 상호작용 친화적인 솔루션을 선택하고 정보 계층 구조가 있는 대화형 인터페이스를 동적으로 조립합니다(물론 터미널과 호환되지 않는 주석도 무시할 수 있습니다). 이 인터페이스는 반드시 읽기 전용 멀티모달 정보가 아니라 대화형 미니 프로그램 본체일 수도 있습니다.
  > 예를 들어, AI가 "이것은 정보 수집입니다"를 이해할 때, 사용자가 일반 텍스트로 질문에 하나씩 답하도록 하는 대신 채팅 인터페이스에 자동으로 채울 수 있는 작은 양식 카드를 삽입할 수 있습니다.

- **컨텍스트 재현**: 컨텍스트 내의 일부 요소를 나타내거나 제어하는 능력을 가집니다. 이것은 일반적으로 여러 애플리케이션 또는 터미널 장치를 동원하는 것을 필요로 합니다. 우리는 일인칭 시점이 안경의 카메라를 통해 재현될 수 있고, 삼인칭 시점이 동반 드론에 의해 제공될 수 있으며, 투영 또는 VR 아이콘이 물리적 객체의 특정 위치를 가리킬 수 있음을 보았습니다... 등등.
  > 예를 들어, 원격 장치 유지보수 시나리오에서 AI는 엔지니어의 AR 시야에서 분해해야 하는 나사 위치를 강조 표시하면서 대형 화면에 회로도 및 단계 지침을 동시에 표시하여 여러 터미널 간에 "컨텍스트"가 공동으로 재현되도록 할 수 있습니다.

#### ❗️❗️ 특별 참고: 중간 언어가 정말 필요한가?

많은 사람들이 중간 언어가 실제로 필요하지 않다고 생각하며, 일반적으로 두 가지 이유가 있습니다:

(1) 장기적으로 AGI는 "행간을 읽는" 능력을 가지고 사용자의 암묵적 의도를 이해합니다. AI가 더 잘 이해하도록 자연어를 인위적으로 불필요하게 처리할 필요가 없습니다.

(2) 인간 친화적인 상호작용 인터페이스 설계는 또한 미래의 AGI의 의무이며, AI는 각 상호작용에 대해 실행 가능한 상호작용 인터페이스를 구체적으로 설계할 수도 있습니다. 따라서 AI의 말을 어떤 중간 언어로 번역하는 것은 더욱 불필요합니다.

그럼에도 불구하고 우리는 최종적으로 iFay 시스템에서 ICP 프로토콜을 설계했습니다. 우리는 다음 3가지 우려를 가지고 있으며 단기적으로 해결하기 어렵다고 믿기 때문에 주석 스타일의 중간 언어를 설계하기로 선택했습니다:

(1) AI의 환경에 대한 통제력이 그렇게 크지 않음

일반적으로 사람들은 인간-AI 상호작용을 사람과 어시스턴트 간의 커뮤니케이션에 비유합니다. 그들은 똑똑한 어시스턴트가 좋은 커뮤니케이션 효과를 달성하기 위해 환경 조건을 적극적으로 조정할 것이라고 생각합니다. 예를 들어, 빛이 부족할 때 조명을 켜고, 문서의 중요한 부분에 표시를 합니다. 그러나 어시스턴트의 권한과 능력이 항상 모든 것을 할 수 있도록 허용하는 것은 아닙니다. 예를 들어, 건물이 갑자기 정전되어 프레젠테이션 슬라이드를 재생할 수 없는 경우와 같습니다.

따라서 더 신중한 접근 방식은 모든 필요한 자료를 준비하고 프레젠테이션을 적응시키는 것입니다(또는 부동산 관리자에게 맡기는 것). 이것은 모든 자료를 가져와 고객을 만나는 것과 같습니다. 고객이 회의실을 가지고 있는지, 프레젠테이션 슬라이드를 재생할 수 있는지, 또는 종이 보고서를 볼 필요가 있는지는 상대방이 결정합니다.

(2) AI와 인간이 그렇게 가깝지 않을 수 있음

AI의 환경에 대한 통제가 제한적이기 때문에 AI는 많은 경우 실제로 인간의 의미를 정말로 이해하지 못합니다. 슬라이드의 데이터 세트를 가리키며 AI에게 묻는 것과 같습니다: "이 데이터는 무엇을 의미합니까?" 실제로 AI는 당신이 어디를 가리키고 있는지 모릅니다. 이상적으로는 AI에게 이 정보를 알려주기 위해 모션 캡처 장비가 필요할 것입니다. 또 다른 시나리오를 상상할 수도 있습니다: 상사가 비공개 회의를 열고, 끝난 후 어시스턴트에게 "회의 결의를 추적하라"고 말합니다. 이 시점에서 어시스턴트는 실제로 일급 정보를 얻지 못했지만, 회의 기록자에 의해 정리된 회의록을 얻었습니다. 회의록은 중간 언어에 의해 처리된 정보와 유사합니다.

따라서 많은 경우 인간에 의해 명시적으로 제공된 정보는 판단에 충분하지 않습니다. 이 시점에서 컨텍스트 정보를 보완해야 하지만, 이것은 특정 AI의 권한이 아닙니다.

(3) 전능한 AGI가 전혀 없을 수 있음

미래의 AI는 인간 사회와 동일한 노동 분할 문제에 반드시 직면할 것입니다. 개별 AI( iFay와 유사)와 사회적 공공 기능을 가진 AI(coFay와 유사)가 존재할 것입니다. 그들 사이에는 필연적으로 권한 경계가 생길 것입니다.

미래의 AI 생태계에서 AI의 책임이 제공된(시스템 입력) 정보를 처리하는 것만인지, 또는 AI가 더 많은 "함의"를 적극적으로 수집하는 책임도 져야 하는지 예측하기 어렵습니다.

따라서 우리는 신중한 접근 방식을 선택합니다. 우리는 AI가 알려진 정보만 처리한다고 가정합니다. 이 정보는 매번 처리 흐름을 통과하며, 이 처리 작업은 소프트웨어, 터미널 장치 또는 AI에 의해 완료될 수 있습니다. 이것은 또한 브라우저를 사용하여 웹사이트에 액세스하는 경우와 같이 현재 엔지니어링 기술 솔루션에서 매우 성숙한 관행이며, 서버는 사용자의 컨텍스트 정보의 일부를 학습할 수 있습니다.

### 🌟 비전

ICP (Interactive Conversation Protocol)는 인간과 기계 간의 중간 언어 형태를 구축하여 인간과 기계 간의 효율적이고 정확하며 풍부한 양방향 통신을 달성하는 것을 목표로 합니다:

#### 인간 → 기계: 표현된 의미와 컨텍스트의 포괄적 복제

- 인간이 표현하는 의미와 컨텍스트를 가능한 한 포괄적으로 포착
- 자연어와 상호작용 의도를 기계가 정확히 이해할 수 있는 구조화된 요소로 변환
- 상호작용의 정확성과 컨텍스트 정보 보존

#### 기계 → 인간: 상호작용 방법의 동적 조립

- 개념 주석을 현재 컨텍스트와 통합
- 장치 기능과 사용자 선호도를 기반으로 가장 적합한 상호작용 방법을 동적으로 조립
- 다중 지각 정보 표현(텍스트, 음성, 시각, 촉각, 후각 등) 지원

