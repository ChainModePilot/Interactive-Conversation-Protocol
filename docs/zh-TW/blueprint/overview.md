# Interactive Conversation Protocol

## 1. 專案定義

Interactive Conversation Protocol（互動式對話協定，簡稱 ICP）是一種用於描述指示性含義的結構化協定。客戶端可根據該協定的結構，將指示性含義組合成人類可識別的模組化多模態訊息。

**核心定位**：面向人機互動的中間語言形態，本質上是一個概念圖的結構化文字表示，我們稱之為「意識註解（Conception Annotation）」。

## 2. 為什麼要創立該專案

### ❓ 要解決的問題

必須面對現實的是，在相當長的時間裡，以什麼樣的方式與用戶接觸是由端側開發者（或公司）決定的。在現有的大多數商業模式下，用戶本人參與交互是產品價值體現和盈利的基礎，比如活躍用戶數和廣告收入。沒有人可以強制要求端側必須開放足夠多的權限，放任在無人介入的情況下完全由 AI 執行操作。

如果 AI 足夠聰明，人類確實沒必要每次都從首頁開始使用產品。因此，我們可以看到人機對話成為下一代的交互主界面幾乎已經成為共識。

然而，自然語言表現力的天然缺陷，原本是寄希望於通過設計好的交互來彌補，現在卻被對話框取代了。對話框的局限性一下子就暴露出來了：

#### (1) 游標的指示作用缺失

交互形式從「螢幕+焦點操作」模式轉向自然語言模式。傳統焦點操作通過鍵盤、滑鼠和觸控螢幕實現，具有精準的指示性。而自然語言交互帶來以下影響：

- **喪失指示的精準性**：表達和理解的難度增大，歧義增多，我們稱之為「游標缺失效應」。
  > 例如，用戶說「刪除這個」時，系統難以確定「這個」具體指向哪個對象，而傳統界面中滑鼠點擊可以精確定位。

- **資訊表達效率受限**：純粹語音的資訊表達效率不高，語音輸入的優勢主要體現在逐字表達場景。
  > 例如，當你想放大一個縮圖時，你可能要說「放大」或者打字「放大」，而傳統的交互只需要點擊一下。

- **語言表達能力的門檻高**：自然語言交互對用戶的文字表達能力要求很高，製造了人機交互的難度。
  > 例如，不擅長文字表達的用戶可能無法準確描述需求，導致系統理解偏差，而傳統界面通過按鈕、選單等視覺元素降低了表達門檻。

- **資訊讀取效率低**：文字流閱讀和聲音的閱讀都不如結構化資訊讀取效率高。
  > 例如，系統用語音播報一長串資料列表時，用戶需要聽完整個列表才能找到目標資訊，而傳統界面可以通過表格、卡片等結構化形式讓用戶快速掃描定位。

- **受制於語輪回合**：受制於語輪回合的交互，對快速連續性操作不夠友好。
  > 例如，用戶需要連續執行多個操作時，必須等待每輪對話完成才能進行下一步，而傳統界面可以快速連續點擊多個按鈕完成批量操作。

#### (2) 資訊碎片氾濫

會話的流式資訊結構缺乏條理，並不像傳統軟體以頁面為單元組織資訊架構，通過可視化的圖形界面構建視覺友好的資訊呈現層次。這就會有以下衍生的問題：

- **不同資訊難以隔離**：在單個會話內連續的資訊流很難辨別不同話題的邊界，甚至多個完全不相干的話題會混雜在一起。
  > 例如，用戶在一個會話中先詢問「幫我查一下明天的天氣」，接著又問「上次那個項目進度怎麼樣了」，然後又問「推薦幾本好書」，這些完全不相關的話題混在一起，難以快速定位和回顧。

- **殭屍會話爆炸**：當人為通過會話隔離資訊時，會話內的資訊被摺疊在以會話為單位的黑盒內，最終因為可見性低而淪為殭屍會話。
  > 例如，用戶創建了「工作相關」、「學習筆記」、「購物清單」等多個會話，但每個會話中只有零散的幾條訊息，時間一長這些會話就被遺忘，成為無法有效利用的殭屍會話。

- **無法多維度管理**：散落在無數會話中的同類資訊組織不起來，因為無法同某個特定維度管理資訊。
  > 例如，用戶在不同會話中分別詢問過「Python 教程」、「JavaScript 教程」、「React 教程」等學習資源，但無法按照「學習資源」這個維度統一查看和管理，只能逐個會話查找。

- **缺少可指示的對象**：資訊溶解在文字資訊中，當我們需要有所指時，沒有具體的對象可指稱。
  > 例如，用戶說「把剛才那個方案再優化一下」，但「剛才那個方案」只是文字流中的一段話，沒有獨立的標識和結構，系統難以精確定位和操作。

#### (3) 不同終端人機界面差異很大

未來更多終端設備將由 Agent 驅動，通過螢幕、攝像頭、麥克風、揚聲器等設備與人的知覺對應，完成人機交互。但是，不同終端因為其物理特徵本身就有差異，不可能強行使用同樣的交互模式。因而在與AI融合上存在難度：

- **媒介斷層**：當AI反饋的資訊結構對終端不友好，那麼必然會造成資訊表達的缺失或混亂。相反，終端提供的資訊結構也不一定是對AI友好的。
  > 例如，把一段原本為大屏儀表盤設計的複雜資料可視化，直接用語音在智能音箱上「念出來」，用戶幾乎無法建立整體認知；反過來，智能手錶上僅有的一行提示資訊，也很難完整承載 AI 期望表達的複雜語義。

- **AI不夠掌握終端特性**：人類為了增強表現力，在複雜語境中或者要表達複雜邏輯時，常常使用多個軟體和終端配合演示。而AI似乎還只會「說」。
  > 例如，產品經理講方案時，會一邊放投影片、一邊在白板上畫結構圖、再配合 Demo 頁面點擊操作；而當前的 AI 往往只能用一段長文字或一串語音來解釋，很難利用投屏、標註、動畫等終端能力來增強表達。

- **虛擬與現實的鴻溝**：目前的AI所使用的語境（或上下文）是基於預設和記憶的知識，而真實場景下的語境往往是動態的、與現實環境相關。
  > 例如，AI 可以「記住」用戶的個人資料和歷史對話，卻很難實時感知用戶此刻坐在會議室、正在翻看哪一頁紙質文件、指向哪一塊實體展板，從而無法像真人助手一樣基於現場情境做出自然的指示和補充。

### 💡 改善思路和目標

以往產品經理最主要的工作就是設計易學易用的界面和操作動線。而在 AI 加持下，用戶不需要再學習軟體的交互界面和操作邏輯，AI 有能力根據用戶的問題和指令只提供給用戶用戶必要的資訊，用戶也只需要做最少的介入操作。

但是只要用戶本人介入，就存在交互友好性、準確性和效率的問題。Interactive Conversation Protocol 正是在人機接觸時發揮作用：

#### 增強自然語言的意義表現力（人 → AI）

這裡說增強意義的表現力，指的是針對自然語言做增強。彌補以上提到的問題（游標指示的缺失、資訊碎片氾濫和不同終端的人機界面的差異）。至少可以對原始自然語言做以下加工：

- **針對表述資訊做標記**：在需要做特別處理的資訊上做標記，這裡所說的特別處理包括採用結構化資訊、組裝界面、運行輔助程式等等。你可以想像成在一段文字上通過圈點來做筆記。標記的形式上我們參考 Markdown，使用特殊的字元表示特定含義，而輔助功能的解釋和觸發參考 Java 開發中的註解原理。通過這種方式，我們可以在原有的闡述性內容中補充說話的語氣、指出什麼是重點、什麼需要特殊的展現形式、什麼需要做前置操作（如認證是本人才可見）。
  > 例如，用戶說「幫我整理下這週的待辦」，在句子裡對日期、優先級、負責人做輕量標記，AI 就可以直接生成一個可勾選的待辦清單，而不是只返回一段描述性的文字。

- **增加語境資訊**：將必要的虛擬的資訊和現實環境補充到敘述的資訊中，以便於重現說話者的真實處境。傳統的交互界面，往往在界面中預置了可選的上下文資訊，以便於從用戶簡單的點擊中就能捕獲用戶的準確意圖，而自然語言如果要完整地描述上下文需要組織冗長的文字。通過在協定中補充時間、地點、設備狀態、參與者身份等語境資訊，AI 可以更精準地理解「此時此地」的真實語義。
  > 例如，用戶只說「在附近訂一家Marry喜歡的餐廳」，則補充定位、預算偏好和歷史訂單等作為語境資訊。語境資訊的應用很廣泛，我們之後還會專門就場景進行討論。

- **轉譯成標準中間語言**：在對原始資訊加工後（添加了註解和語境資訊）後，為了能夠完整且準確地進行解讀，必然需要一個約定好的資料標識系統。為了適應所有終端的表現力，這個標識系統可以建立在 JSON 的規範上，提供約定好的參數表和結構體。這樣以來，各個接收端的 AI 可以調動所有可用的終端展現最大化的表現力，重現表達者的完整意義。
  > 例如，一句「把這段話發給項目群並讓大家在今天下班前確認」最終被轉譯成一個包含訊息體、接收人列表、截止時間、確認按鈕配置的標準 JSON 結構，聊天工具、Web 後台或移動 App 都可以據此渲染出各自適配的界面。

#### 按需定制界面（AI → 人）

我們的前提是人們會更喜歡通過「說」與AI交互，這是最接近於人的溝通方式。因此，人們會越來越覺得通過點擊尋找自己需要的功能界面太麻煩。而人們需要的資訊也好，界面也好都是應該被直接推到用戶「眼前」。為了有這樣的效果，在接收端應該有一定的解讀能力：

- **解讀中間語言**：由於中間語言是 JSON 格式，所有接收端都可以讀取到完整的語義，至少不會在資訊接收上產生斷層。
  > 例如，同一份「報銷單審核請求」的中間語言資料，桌面端可以渲染為帶有表格和附件預覽的大屏界面，手機端只展示關鍵資訊和兩個按鈕（通過/退回），而智能音箱可以用語音念出摘要並等待語音確認。

- **動態構建訊息界面**：根據完整的語境和註解，選擇交互最為友好的方案，動態地拼裝一個有資訊層次的交互界面（當然也可以忽略與終端不適配的註解）。這個界面不一定是只讀的多模態資訊，也可以是一個可以交互的小程式體。
  > 例如，當 AI 理解到「這是一次資訊收集」時，可以自動在聊天界面中插入一個可填寫的小表單卡片，而不是讓用戶在純文字中逐條回答問題。

- **重現語境**：有能力指示或支配語境中的一些元素。這通常需要調動多個應用或終端設備。我們已經看到，可以通過眼鏡上的攝像頭重現第一視角，通過伴飛無人機充當第三視角，利用投影或者 VR 圖標指向實物上的某個位置……等等。
  > 例如，在遠程設備維修場景中，AI 可以一邊在工程師眼前的 AR 視野中高亮需要拆卸的螺絲位置，一邊在大屏上同步顯示電路圖和步驟說明，讓「語境」在多個終端之間被共同重現出來。

#### ❗️❗️ 特別說明：中間語言真的必要麼

很多人都認為其實不需要中間語言，一般有兩個理由：

（1）長遠來看，AGI 有「察言觀色」的能力，然後理解用戶的隱含意圖。並不需要為了讓 AI 更好地理解，人為地對自然語言做多餘的加工。

（2）設計對人友好的交互界面未來也是 AGI 的本分，甚至在每一次互動中 AI 都可能會針對性設計一個可運行的交互界面。因此，更加不需要把 AI 的話再翻譯成某個中間語言。

我們最終仍然在 iFay 的體系中設計了 ICP 協定，我們有以下 3 個顧慮，並且認為短期內很難解決，因而選擇設計一個註解式的中間語言：

（1）AI 對環境的掌控力沒那麼大

一般人會把人跟 AI 的交互類比為一個人跟助理的溝通。認為一個聰明的助理會為了達到好的溝通效果主動調整環境的條件，比如光線不夠的時候會主動打開燈；在文件重要的地方做記號。但是助理的權限和能力並不總是可以做所有事，比如大樓突然停電了，沒辦法播放演示文稿。

因此，一個更穩妥的做法是準備好所有的必要資料，展示的事隨機應變（或者說交給物業的管理者）。這就像我們帶上所有資料去見客戶，至於客戶有沒有會議室，能不能播放演示文稿，還是要看紙質報告，由對方決定。

（2）AI 跟人可能沒那麼親近

因為 AI 對環境掌控力有限，以至於 AI 在很多情況下實際上並沒有真的讀懂人的意思。就好像你指著幻燈片上的一組資料問 AI：「這資料是什麼意思？」 其實 AI 並不知道你指在哪裡。理想狀況下，這個時候就需要動作捕捉設備將這個資訊告訴 AI。你還可以想像另一個場景，老闆開閉門會議，開完後跟助理說：「跟進一下會議決議。」 這時候助理實際上並沒有獲得第一手資訊，而是經過會議記錄員整理的會議紀要。會議紀要就類似中間語言加工後的資訊。

因此，很多時候人明示的資訊本身就不足以做判斷。這個時候就需要補充語境的資訊，但這並不是某個特定 AI 的職權。

（3）可能根本就沒有萬能的 AGI

未來 AI 一定會遇到跟人類社會一樣的分工問題。會存在個體的 AI（類似 iFay），也會存在具有社會公共職能的 AI（類似 coFay）。他們之間必然會產生權限邊界。

我們很難預測在未來的 AI 生態裡，AI 的職責是僅對提供的（系統輸入的）資訊做處理，還是 AI 也要負責主動的去收集更多的「言外之意」。

所以我們選擇穩妥的做法，我們假設 AI 只對已獲知的資訊做處理。只不過這個資訊每次都走一個加工流程，這個加工動作可能是某個軟體、終端設備或者 AI 完成的。這也是現在工程技術方案中很成熟的做法，比如使用瀏覽器訪問網站，服務端能夠獲知一部分用戶的上下文資訊。

### 🌟 願景

ICP（Interactive Conversation Protocol） 旨在構建一種跨人機的中間語言形態，實現人機之間的高效、準確、豐富的雙向溝通：

#### 人 → 機器：全面複刻表達意義和語境

- 盡可能全面地捕獲人表達的意義和語境
- 將自然語言和交互意圖轉化為機器可準確理解的結構化要素
- 保留交互的精準性和上下文資訊

#### 機器 → 人：動態組裝可交互方式

- 將概念註解與當前語境融合
- 根據設備能力和用戶偏好，動態組裝最適合的交互方式
- 支援多知覺資訊呈現（文字、語音、視覺、觸覺、嗅覺等）

